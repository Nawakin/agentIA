# /home/sebastien/IA_AGENT/agent.py

from github_integration import GitHubIntegration
from project_analyzer import ProjectAnalyzer
from config import MODEL_PATH
from transformers import LlamaTokenizer, LlamaForCausalLM,PreTrainedTokenizerFast
import json,os,sys,time

class Agent:
    def __init__(self):
        self.github = GitHubIntegration()
        self.project_analyzer = ProjectAnalyzer(self.github)
        if not os.path.exists(MODEL_PATH):
            raise ValueError(f"Le r√©pertoire du mod√®le n'existe pas √† {MODEL_PATH}")
        # Chargement du mod√®le LLaMA
        self.model = LlamaForCausalLM.from_pretrained(MODEL_PATH,local_files_only=True,revision=None)
        #self.tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH,local_files_only=True,legacy=False)
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_PATH, local_files_only=True)
        self.tokenizer.add_special_tokens({'pad_token':'<PAD>'})
        self.tokenizer.pad_token = "<PAD>"
        self.tokenizer.pad_token_id = self.tokenizer.convert_tokens_to_ids("<PAD>") 
        self.model.eval()  # Mode √©valuation pour l'inf√©rence

    def generate_code(self, prompt):
        """ G√©n√®re du code en r√©ponse √† un prompt donn√© en utilisant le mod√®le LLaMA """
        print("\nüì§ Prompt envoy√© au mod√®le :\n" + prompt)
        print("\n‚è≥ G√©n√©ration en cours...", end="", flush=True)
         # Tokenisation du prompt
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
        attention_mask = inputs.attention_mask
        # Extraction des ids de tokens pour l'entr√©e
        input_ids = inputs["input_ids"]

        eos_token_id = self.model.config.eos_token_id
        outputs = self.model.generate(
            input_ids,  # Passer les identifiants de tokens
            attention_mask=attention_mask,
            max_new_tokens=400,
            temperature=0.3,  # Contr√¥le la cr√©ativit√©, un peu plus al√©atoire
            top_p=0.5,  # Utilise la probabilit√© cumulative pour g√©n√©rer des mots
            no_repeat_ngram_size=3,  # √âvite la r√©p√©tition de n-grammes
            pad_token_id=eos_token_id,  # Assure une gestion correcte des tokens de fin
            num_return_sequences=1
        )
        
        # D√©codage des sorties en texte
        generated_code = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        if generated_code.startswith(prompt):
            generated_code = generated_code[len(prompt):].strip()
        if prompt in generated_code:
            generated_code = generated_code.replace(prompt, '').strip()
        # Supprime l'animation de chargement et affiche la r√©ponse
        print("\r‚úÖ R√©ponse re√ßue !")
        print("\nüì• R√©ponse brute du mod√®le :\n" + generated_code) 
        return generated_code

    def correct_code(self, file_path, content):
        """ Corrige un fichier en tenant compte des erreurs d'import et de coh√©rence """
        # Analyser les imports et les classes du projet
        imports = self.project_analyzer.get_imports()
        structure = self.project_analyzer.get_project_structure()
        
        # G√©n√©rer un prompt bas√© sur le contenu du fichier et les erreurs possibles
        prompt = f"Corrige les erreurs d'import et de coh√©rence dans ce fichier :\n{content}\n\nStructure du projet : {structure}\nImports : {imports}"
        
        # G√©n√©rer le code corrig√©
        corrected_code = self.generate_code(prompt)
        
        # Mettre √† jour le fichier sur GitHub
        self.github.update_file(file_path, corrected_code, f"Correction de {file_path}")

    def process_consigne(self, consigne):
        """ Traite la consigne donn√©e (ex. corriger les erreurs de code ou impl√©menter une fonctionnalit√©) """
        # Test pr√©liminaire : envoyer une question au LLM
        test_prompt = "Comment vas-tu ? R√©pond en fran√ßais."
        print("üì§ Envoi du prompt au LLM : ", test_prompt)
        # Envoi du prompt et r√©cup√©ration de la r√©ponse
        response = self.generate_code(test_prompt)
        # Afficher la r√©ponse dans la console
        print("üì• R√©ponse du LLM : ", response)
 
        if "corrige" in consigne:
            # Analyser tous les fichiers du projet pour trouver les erreurs
            print("Corriger les erreurs de coh√©rence et d'import...")
            files = self.github.list_files("/")
            for file_path in files:
                if file_path.endswith((".js", ".ts", ".tsx")):
                    content = self.github.get_file_content(file_path)
                    self.correct_code(file_path, content)
        elif "code moi" in consigne:
            # Impl√©menter une nouvelle fonctionnalit√© d√©crite dans dev.txt
            print("Impl√©mentation d'une fonctionnalit√©...")
            self.implement_feature_from_dev()

    def implement_feature_from_dev(self):
        """ Impl√©mente la premi√®re fonctionnalit√© non r√©alis√©e dans dev.txt """
        dev_file_path = 'dev.txt'
        dev_content = self.github.get_file_content(dev_file_path)
        features = json.loads(dev_content)
        for feature in features["fonctionnalites"]:
            if not feature['developpe']:
                print(f"Impl√©mentation de la fonctionnalit√©: {feature['description']}")
                # Construire le prompt pour demander au mod√®le de g√©n√©rer les fichiers √† modifier et le code √† ins√©rer
                prompt = f"""
Voici les fichiers du projet et leurs contenus: {self.project_analyzer.get_project_structure()}.
Voici la description de la fonctionnalit√© : {feature['description']}.

Tu dois g√©n√©rer un **JSON strictement valide** contenant la liste des fichiers √† modifier ou √† cr√©er, et le code √† ins√©rer dans chaque fichier.  
Le format attendu est :
```json
{{
  "fichiers": [
    {{
      "nom": "src/components/PlayButton.js",
      "code": "import React from 'react';\nexport default function PlayButton() {{ return <button>Jouer</button>; }}"
    }},
    {{
      "nom": "src/screens/ShopScreen.js",
      "code": "import React from 'react';\nexport default function ShopScreen() {{ return <div>Boutique</div>; }}"
    }}
  ]
}}
Ne g√©n√®re rien d'autre que ce format JSON valide. """
                # G√©n√©rer la r√©ponse √† partir du mod√®le
                response = self.generate_code(prompt)
                # Analyser la r√©ponse du mod√®le pour extraire les fichiers et le code
                try:
                    modification_data = json.loads(response)
                    self.apply_modifications(modification_data)
                    print(f"‚úÖ Fonctionnalit√© impl√©ment√©e avec succ√®s.")
                except json.JSONDecodeError:
                    print("‚ùå Erreur lors de la parsing de la r√©ponse JSON du mod√®le")
                break  # Quitte la boucle apr√®s avoir trait√© la premi√®re fonctionnalit√© non d√©velopp√©e
    def apply_modifications(self, modification_data):
        """ Applique les modifications sur le projet en fonction de la r√©ponse JSON du mod√®le """
        for modification in modification_data['fichiers']:
            file_path = modification['nom']
            new_code = modification['code']
            
            # Mettre √† jour le fichier sur GitHub
            self.github.update_file(file_path, new_code, f"Ajout de la fonctionnalit√© dans {file_path}")

